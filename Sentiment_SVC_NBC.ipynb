{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM97qsFBmmcVWyOH9HegU4j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KanmaniKannathasan/DataMining/blob/main/Sentiment_SVC_NBC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pku78H-vXL2Y"
      },
      "outputs": [],
      "source": [
        "import re, nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import joblib\n",
        "\n",
        "# Reading dataset as dataframe\n",
        "df = pd.read_csv(\"Tweet_Data.csv\")\n",
        "pd.set_option('display.max_colwidth', None) # Setting this so we can see the full content of cells\n",
        "pd.set_option('display.max_columns', None) # to make sure we can see all the columns in output window\n",
        "\n",
        "# Cleaning Tweets\n",
        "def cleaner(tweet):\n",
        "    soup = BeautifulSoup(tweet, 'lxml') # removing HTML entities such as ‘&amp’,’&quot’,'&gt'; lxml is the html parser and shoulp be installed using 'pip install lxml'\n",
        "    souped = soup.get_text()\n",
        "    re1 = re.sub(r\"(@|http://|https://|www|\\\\x)\\S*\", \" \", souped) # substituting @mentions, urls, etc with whitespace\n",
        "    re2 = re.sub(\"[^A-Za-z]+\",\" \", re1) # substituting any non-alphabetic character that repeats one or more times with whitespace\n",
        "\n",
        "    \"\"\"\n",
        "    For more info on regular expressions visit -\n",
        "    https://docs.python.org/3/howto/regex.html\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = nltk.word_tokenize(re2)\n",
        "    lower_case = [t.lower() for t in tokens]\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n",
        "\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
        "    return lemmas\n",
        "\n",
        "df['cleaned_tweet'] = df.full_text.apply(cleaner)\n",
        "df = df[df['cleaned_tweet'].map(len) > 0] # removing rows with cleaned tweets of length 0\n",
        "print(\"Printing top 5 rows of dataframe showing original and cleaned tweets....\")\n",
        "print(df[['full_text','cleaned_tweet']].head())\n",
        "df.drop(['id', 'created_at', 'full_text'], axis=1, inplace=True)\n",
        "# Saving cleaned tweets to csv\n",
        "df.to_csv('cleaned_data.csv', index=False)\n",
        "df['cleaned_tweet'] = [\" \".join(row) for row in df['cleaned_tweet'].values] # joining tokens to create strings. TfidfVectorizer does not accept tokens as input\n",
        "data = df['cleaned_tweet']\n",
        "Y = df['sentiment'] # target column\n",
        "tfidf = TfidfVectorizer(min_df=.00015, ngram_range=(1,3)) # min_df=.00015 means that each ngram (unigram, bigram, & trigram) must be present in at least 30 documents for it to be considered as a token (200000*.00015=30). This is a clever way of feature engineering\n",
        "tfidf.fit(data) # learn vocabulary of entire data\n",
        "data_tfidf = tfidf.transform(data) # creating tfidf values\n",
        "pd.DataFrame(pd.Series(tfidf.get_feature_names_out())).to_csv('vocabulary.csv', header=False, index=False)\n",
        "print(\"Shape of tfidf matrix: \", data_tfidf.shape)\n",
        "\n",
        "# Implementing Support Vector Classifier\n",
        "svc_clf = LinearSVC() # kernel = 'linear' and C = 1\n",
        "\n",
        "# Running cross-validation\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) # 10-fold cross-validation\n",
        "scores=[]\n",
        "iteration = 0\n",
        "for train_index, test_index in kf.split(data_tfidf, Y):\n",
        "    iteration += 1\n",
        "    print(\"Iteration \", iteration)\n",
        "    X_train, Y_train = data_tfidf[train_index], Y[train_index]\n",
        "    X_test, Y_test = data_tfidf[test_index], Y[test_index]\n",
        "\n",
        "    svc_clf.fit(X_train, Y_train) # Fitting SVC\n",
        "    Y_pred = svc_clf.predict(X_test)\n",
        "    score = metrics.accuracy_score(Y_test, Y_pred) # Calculating accuracy\n",
        "    print(\"Cross-validation accuracy: \", score)\n",
        "    scores.append(score) # appending cross-validation accuracy for each iteration\n",
        "svc_mean_accuracy = np.mean(scores)\n",
        "print(\"Mean cross-validation accuracy: \", svc_mean_accuracy)\n",
        "\n",
        "# Implementing Naive Bayes Classifier\n",
        "nbc_clf = MultinomialNB()\n",
        "\n",
        "# Running cross-validation\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) # 10-fold cross-validation\n",
        "scores=[]\n",
        "iteration = 0\n",
        "for train_index, test_index in kf.split(data_tfidf, Y):\n",
        "    iteration += 1\n",
        "    print(\"Iteration \", iteration)\n",
        "    X_train, Y_train = data_tfidf[train_index], Y[train_index]\n",
        "    X_test, Y_test = data_tfidf[test_index], Y[test_index]\n",
        "    nbc_clf.fit(X_train, Y_train) # Fitting NBC\n",
        "    Y_pred = nbc_clf.predict(X_test)\n",
        "    score = metrics.accuracy_score(Y_test, Y_pred) # Calculating accuracy\n",
        "    print(\"Cross-validation accuracy: \", score)\n",
        "    scores.append(score) # appending cross-validation accuracy for each iteration\n",
        "nbc_mean_accuracy = np.mean(scores)\n",
        "print(\"Mean cross-validation accuracy: \", nbc_mean_accuracy)\n"
      ]
    }
  ]
}