{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNS6DG3NxSGSsemgkfojJ+D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KanmaniKannathasan/DataMining/blob/main/Sentiment_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc3yl-W3WkvD"
      },
      "outputs": [],
      "source": [
        "import re, nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import joblib\n",
        "\n",
        "model = joblib.load('svc.sav')\n",
        "vocabulary = pd.read_csv('vocabulary.csv', header=None)\n",
        "vocabulary_dict = {}\n",
        "for i, word in enumerate(vocabulary[0]):\n",
        "      vocabulary_dict[word] = i\n",
        "print(vocabulary_dict)\n",
        "tfidf = TfidfVectorizer(vocabulary = vocabulary_dict,lowercase=False)\n",
        "\n",
        "# Reading new data as dataframe\n",
        "df = pd.read_csv(\"trump_tweets.csv\")\n",
        "pd.set_option('display.max_colwidth', None) # Setting this so we can see the full content of cells\n",
        "pd.set_option('display.max_columns', None) # to make sure we can see all the columns in output window\n",
        "\n",
        "# Cleaning Tweets\n",
        "def cleaner(tweet):\n",
        "    soup = BeautifulSoup(tweet, 'lxml') # removing HTML entities such as ‘&amp’,’&quot’,'&gt'; lxml is the html parser and shoulp be installed using 'pip install lxml'\n",
        "    souped = soup.get_text()\n",
        "    re1 = re.sub(r\"(@|http://|https://|www|\\\\x)\\S*\", \" \", souped) # substituting @mentions, urls, etc with whitespace\n",
        "    re2 = re.sub(\"[^A-Za-z]+\",\" \", re1) # substituting any non-alphabetic character that repeats one or more times with whitespace\n",
        "\n",
        "    \"\"\"\n",
        "    For more info on regular expressions visit -\n",
        "    https://docs.python.org/3/howto/regex.html\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = nltk.word_tokenize(re2)\n",
        "    lower_case = [t.lower() for t in tokens]\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n",
        "\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
        "    return lemmas\n",
        "\n",
        "df['cleaned_tweets'] = df.tweets.apply(cleaner)\n",
        "df = df[df['cleaned_tweets'].map(len) > 0] # removing rows with cleaned tweets of length 0\n",
        "print(\"Printing top 5 rows of dataframe showing original and cleaned tweets....\")\n",
        "print(df[['tweets','cleaned_tweets']].head())\n",
        "df['cleaned_tweets'] = [\" \".join(row) for row in df['cleaned_tweets'].values] # joining tokens to create strings. TfidfVectorizer does not accept tokens as input\n",
        "data = df['cleaned_tweets']\n",
        "print(df['cleaned_tweets'].head())\n",
        "tfidf.fit(data)\n",
        "data_tfidf = tfidf.transform(data)\n",
        "y_pred = model.predict(data_tfidf)\n",
        "#### Saving predicted sentiment of tweets to csv\n",
        "df['predicted_sentiment'] = y_pred.reshape(-1,1)\n",
        "df.drop(['id', 'created_at'], axis=1, inplace=True)\n",
        "df.to_csv('predicted_sentiment.csv', index=False)\n"
      ]
    }
  ]
}